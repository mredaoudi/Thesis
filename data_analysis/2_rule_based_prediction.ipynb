{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "487d80ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohammedredadaoudi/thesis/venv/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "from vaderSentiment_fr.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3008a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIA = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b52dcf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(dataset_name):\n",
    "    return json.load(open('../data/' + dataset_name + '.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "600d761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet_text):\n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c0206e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tweets(dataset_name):\n",
    "    dataset = load_json_data(dataset_name)\n",
    "    load = {}\n",
    "    for tweet, details in dataset.items():\n",
    "        if details['tweet_data']:\n",
    "            load[tweet] = preprocess_tweet(details['tweet_data']['text'])\n",
    "        else:\n",
    "            load[tweet] = details['previous_processed_text']\n",
    "    return load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "792c34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_rule_based(tweet_text):\n",
    "    vader_scores = SIA.polarity_scores(tweet_text)\n",
    "    textblob_scores = TextBlob(tweet_text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment\n",
    "    if vader_scores['compound'] > 0.05:\n",
    "        vader_label = 'pos'\n",
    "    elif vader_scores['compound'] < -0.05:\n",
    "        vader_label = 'neg'\n",
    "    else:\n",
    "        vader_label = 'neu'\n",
    "    if textblob_scores[0] > 0.1:\n",
    "        textblob_label = 'pos'\n",
    "    elif textblob_scores[0] < -0.1:\n",
    "        textblob_label = 'neg'\n",
    "    else:\n",
    "        textblob_label = 'neu'\n",
    "    return (vader_label, textblob_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "072f3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_sentiment():\n",
    "    dataset = extract_tweets('training')\n",
    "    training = load_json_data('training')\n",
    "    #tweet_ids = []\n",
    "    tweet_texts = []\n",
    "    training_labels = []\n",
    "    \n",
    "    for tweet in dataset:\n",
    "        #tweet_ids.append(tweet)\n",
    "        tweet_texts.append(dataset[tweet])\n",
    "        training_labels.append(training[tweet]['sentiment_label'])\n",
    "    #return pd.DataFrame({'Tweet ID': tweet_ids,\n",
    "    #                     'Tweet Text': tweet_texts,\n",
    "    #                     'Training Labels': training_labels})\n",
    "    return pd.DataFrame({'Training Labels': training_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4a319702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_based_sentiment(dataset_name):\n",
    "    dataset = extract_tweets(dataset_name)\n",
    "    #tweet_ids = []\n",
    "    tweet_texts = []\n",
    "    vader_labels = []\n",
    "    textblob_labels = []\n",
    "    \n",
    "    for tweet in dataset:\n",
    "        #tweet_ids.append(tweet)\n",
    "        tweet_texts.append(dataset[tweet])\n",
    "        rule_based_labels = get_labels_rule_based(dataset[tweet])\n",
    "        vader_labels.append(rule_based_labels[0])\n",
    "        textblob_labels.append(rule_based_labels[1])\n",
    "        \n",
    "    #return pd.DataFrame({'Tweet ID': tweet_ids,\n",
    "    #                     'Tweet Text': tweet_texts,\n",
    "    #                     'VADER Labels': vader_labels,\n",
    "    #                     'TextBlob Labels': textblob_labels})\n",
    "    return pd.DataFrame({'VADER Labels': vader_labels,\n",
    "                         'TextBlob Labels': textblob_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d93e4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_training = training_sentiment()\n",
    "rule_based_training = rule_based_sentiment('training')\n",
    "rule_based_prediction_macron = rule_based_sentiment('prediction_macron')\n",
    "rule_based_prediction_lepen = rule_based_sentiment('prediction_lepen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2d7a2c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pd.concat([real_training, rule_based_training], axis=1)\n",
    "joined = joined.loc[:,~joined.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c375d9eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rule_based_training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrule_based_training\u001b[49m[rule_based_training\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mmin\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mmax\u001b[39m(x), \u001b[38;5;241m1\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rule_based_training' is not defined"
     ]
    }
   ],
   "source": [
    "rule_based_training[rule_based_training.apply(lambda x: min(x) == max(x), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2b177",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
